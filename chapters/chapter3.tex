\section{Chapter 3}


\begin{definition}[\bfseries subspace]

  a subset $W$ of $\mathbb{R}^{n}$ is a subspace if it is not empty,
  $w_{1}\cdots, w_{n}$ are elements of $W$ and $c_{1}\cdots c_{n}$ are scalars,
  the linear combination $c_{1}w_{1}\cdots + c_{n}w_{n}$ is also in $W$.
  A subspace is \textbf{proper} if it is not trivial (${0}$) nor the entire
  $\mathbb{R}^{n}$.

\end{definition}

\begin{definition}[\bfseries nullspace]

  Subspace consisting of $\vec{x}$ that solve $AX=0$.

\end{definition}

\begin{definition}[\bfseries field]

  A field $F$ is a set together with two laws of composition,
  addition and multiplication which satisfy the following axioms:
  \begin{itemize}
    \item Addition makes F into an abelian group with identity 0
    \item multiplication makes the set of $\textbf{nonzero}$ elements
      of $F$ into an abelian group; identity is 1
    \item distributive law: $a(b+c)=ab + ac$

  \end{itemize}

  The \textbf{characteristic} is the number of times you have to add $1$ together
  to get $0$. In the field of complex numbers, the characteristic is infinite
  and therefore \textbf{characteristic zero}. In $\mathbb{F}_{p}$, the
  characteristic is $p$.

  Note that the characteristic of any field is either zero (infinite) or some
  prime, $p$. (simple proof by contradiction)


\end{definition}


\begin{definition}[\bfseries prime field]

  For some prime p, the congruence classes define a field

  \[\mathbb{F}_{p}=\{\bar{0},\cdots \overline{p-1}\}=\mathbb{Z}/p\mathbb{Z}\]

  The multiplicative group $\mathbb{F}_{p}^{\times}$ of the prime field is a
  cyclic group of order $p-1$. (no proof provided\ldots yet). A generator for this
  cyclic group is called a $\textbf{primitive root}$ modulo $p$. This lets you generate
  the entire gruop from this element $\alpha$. E.g.
  \[\mathbb{F}_{p}^{\times}=\{1, 2,\ldots , p-1\}=\{1, \alpha, \ldots , \alpha^{{p-2}}\}\]

\end{definition}


\begin{definition}[\bfseries Cancellation law (prime field)]

  I feel that this is rather obvious (and the proof is trivial), but im taking
  note of it anyway. Let $p$ be a prime integer and let $\bar{a},\bar{b}$, and
  $\bar{c}$ be elements of $\mathbb{F}_{p}$. Then:
  \begin{itemize}
      \item if $\bar{a}\bar{b}=\bar{0}$ then $\bar{a}$ or $\bar{b}$ is $\bar{0}$.
      \item if $\bar{a}\neq \bar{0}$ and $\bar{a}\bar{b}=\bar{a}\bar{c}$, then
      $\bar{b} = \bar{c}$.
  \end{itemize}

\end{definition}

\begin{definition}[\bfseries Vector Space over a field]
  A \textbf{vector space} $V$ over a field $F$ is a set together with two laws
  of composition:
  \begin{itemize}
    \item addition: $V \times V \rightarrow V$, or for specific elements,
      $v, w \rightsquigarrow v + w$.
    \item scalar multiplication by elements of the field: $F \times V \rightarrow V$
      , or for specific elements, $c, v \rightsquigarrow cv$, for $c$ in $F$ and
      $v$ in $V$.
  \end{itemize}

  The following axioms hold:
  \begin{itemize}
    \item addition over $V$ is an abelian group with identity $0$
    \item $1v = v \forall v \in V$
    \item associativity: $(ab)v = a(bv)\forall a, b \in F \times F, v \in V$.
    \item distributivity:
      $(a+b)v = av + bv$, $a(v+w)=av + aw \forall a, b \in F \times F, v \in V$

  \end{itemize}

\end{definition}

\begin{definition}[\bfseries Vector Subspace over a field]
  Analagous to vector subspace. E.g. closed over addition and scalar
  multiplication. Same idea as vector subspace being proper (not id or entire thing).
\end{definition}

\begin{definition}[\bfseries isomorphism between vector spaces over same field]
  An isomorphism $\phi$ from a vector space $V$ to a vector space $V'$, both over
  the same field, $F$, is a bijective map $V \rightarrow V'$ with two laws of composition:

  \[\forall v,w \in V \times V, c \in F: \phi ( v + w ) = \phi (v) + \phi (w), \phi (cv) = c \phi (v)\]

\end{definition}

\begin{definition}[\bfseries Ordered Set]
  denoted with parentheses instead of brackets. Order matters. Can include
  duplicate elements. This ordered set (when it contains vectors) can span a
  subspace.
\end{definition}

\begin{definition}[\bfseries Linear Combination]

  Let $V$ be a vector space over a field $F$. Let $S$ be an ordered set of
elements of $V$: $(v_{1},\ldots, v_{n})$ (denoted hypervector). A \textbf{linear
combination} of $S$ is a vector of the form:
  \[w = c_{1}v_{1}\ldots c_{n}v_{n}, c_{i} \in F\]

\end{definition}

\begin{definition}[\bfseries independence]

  An ordered set of vectors, $S$, is \textbf{linearly independent} if there is
no linear relation $SX = 0$ except for the trivial one. A set that is not
independent is dependent.

\end{definition}

\begin{definition}[\bfseries Span]

  Let $V$ be a subspace. $Span S$ is the smallest subspace of $V$ that contains
  $S$.

\end{definition}

\begin{definition}{\bfseries basis}

  The basis of a vector space $V$ is a set of vectors that is independent and
  also spans $V$.

  Standard basis of $F_{n}$ is the set $\textbf{E}=(e_{1}\ldots e_{n})$ ($e_{i}$
is 1 in ith entry and 0 everywhere else).

\end{definition}

\begin{definition}{\bfseries dimension of basis}

  The dimension of a finite-dimensional (e.g. finite) vector space $V$ is the
  order of a basis in $V$.

\end{definition}

\begin{definition}[\bfseries a theorem\ldots]

  Let $S$ and $L$ be finite subsets of a vector space $V$. Assume that $S$ spans
  $V$ and that $L$ is independent. Then $S$ contains at least as many elements
  as $L$ does: $|S| \geq |L|$.

\end{definition}

\begin{definition}[\bfseries Coordinate vector]

  If we have some basis $\textbf{B} = (v_{1},\ldots v_{n})$ of vector space $V$
  over field $F$, then the coordinates of some vector $v$ in the vector space is
  characterized by the coordinate vector of $v$, X (a vector of scalars from
  $F$), with respect to basis $\textbf{B}$:
\[v = \textbf{B}X^{T}\]

\end{definition}

\begin{definition}[\bfseries basechange matrix]

  We have two bases in the same vector space $V$: $\textbf{B} = (v_{1}, \ldots v_{n}),\textbf{B}'= (v_{1}'\ldots v_{n})$. We want to figure out how to go from $B$ to $B'$. Then for every element in $\textbf{B}'$:

  \[v_{j}'=v_{1}p_{1j}+\ldots v_{n}p_{nj}\]

  You can do this for all the elements in $B'$, then put them into the columns
of a matrix ($v_{j}'$ is the jth column). This matrix, $P$, satisfies
$\textbf{B'} = \textbf{B}P$ and is the basechange matrix.

\end{definition}

\begin{definition}[\bfseries Sum of Subspaces]
  Also denoted span of subspaces. Analagous to span of a set of vectors. It's

  the smallest subspace that contains all vectors in all subspaces.

\end{definition}

\begin{definition}[\bfseries Independence of vector subspaces]

  Similar to independence in a set of vectors. If you can't make zero by adding
  vectors up (except trivially), then are independent.

\end{definition}

\begin{definition}[\bfseries some subspace property]

  Suppose we have $k$ subspaces of a vector space. Then:
  \[\dim (W_{1}\ldots W_{k}) \leq dim W_{1} + \ldots + W_{k}\]

  Intuitively, this is obvious, because the size of the bases are going to be
  bigger when separated, and equal when they are independent.

\end{definition}

\begin{definition}[\bfseries direct sum]

  Suppose we have $k$ subspaces of a vector space V: $W_{1}\ldots W_{k}$. Assume
that the subspaces are independent of each other, and that their sum is equal to
$V$ (in the sense that it spans it). Then, we say that $V$ is the direct sum of
$W_{1}\ldots W_{k}$. We write $V$ as $V=W_{1}\oplus \ldots W_{k}$.

\end{definition}
